# Smart-Bin-Classifier

A multimodal AI system that verifies whether the contents of a warehouse bin image match a customer’s order.

- **Computer Vision + NLP + Numeric reasoning**
- **Model:** CLIP-based image–text matcher + MLP for quantity
- **Frontend:** Streamlit web app
- **Backend:** PyTorch, Hugging Face `transformers`

---

**Dataset**: [Amazon Bin-Image Dataset](https://registry.opendata.aws/amazon-bin-imagery/)

**Deployed Streamlit Websit Link**: [Streamlit Website](https://smart-bin-classifier-iwiks75ayyfsekvms8nxgz.streamlit.app)

**EDA Code and Documentation** can be found here [here](./EDA)

**Training Code** can be found [here](./trainer)

---
## 1. Problem Statement & Objectives

Modern e-commerce fulfillment centers pack multiple items into a single bin. Human or rule-based systems must ensure that:

> **The items and their quantities inside the bin match the customer’s order or invoice.**

This project addresses the following core objective:

> Given a **bin image** and a **customer order** (items + quantities), determine **for each ordered item** whether the bin contains **at least the requested quantity** of that item.

---

## 2. Dataset & Preprocessing

> ⚠️ Note: This project uses a **subset** of the original dataset for practicality (e.g., ~25k files).
> The [downloader](./downloader/downloader.py) was used to download the subset of dataset for training. 

### 2.1 Source Data

Each bin is represented by:

- **Bin image:** an RGB image showing multiple packed items.
- **Metadata JSON:** structured like:

```json
{
  "BIN_FCSKU_DATA": {
    "B00S81WTMA": {
      "asin": "B00S81WTMA",
      "name": "...",
      "normalizedName": "...",
      "quantity": 2,
      "height": {...},
      "length": {...},
      "width": {...},
      "weight": {...}
    },
    "B014F6ODIY": {
      "asin": "B014F6ODIY",
      "name": "...",
      "normalizedName": "...",
      "quantity": 1,
      ...
    }
  },
  "EXPECTED_QUANTITY": 3
}
```

### 2.2 Metadata Parsing

We build a **flattened item-level table** df_items:

| image_id | asin       | item_name                                      | bin_quantity |
| -------- | ---------- | ---------------------------------------------- | ------------ |
| 12345    | B00S81WTMA | MelodySusie 24W LED Nail Dryer ...             | 2            |
| 12345    | B014F6ODIY | Deconovo Thermal Insulated Grommet Blackout... | 1            |

Processing details:

	•	Extract BIN_FCSKU_DATA for each JSON file.
	•	Construct item_name from name or normalizedName (fallback to asin if needed).
	•	Skip entries where both name and normalizedName are missing/empty.
	•	Only retain image_id, asin, item_name, bin_quantity.
	•	Optionally, drop bin images with no valid items.

#3. Methodology & Approach

### 3.1 Why Not Multi-class Detection?
	•	The number of unique ASINs/items is very high (~38k+).
	•	The provided dataset metadata covers only a subset of items.
	•	Annotated bounding boxes are not provided.
	•	Training a multi-class object detector (YOLO/Faster-RCNN) for all ASINs is impractical.

### 3.2 Reformulating the Task

Instead of “classifying the whole bin”, we treat the problem as:

A binary decision per (bin image, item name, requested quantity).

For each tuple (image, item_name, required_quantity):

	•	Label = 1 if the bin contains that item with bin_quantity >= required_quantity.
	•	Label = 0 otherwise (wrong item or insufficient quantity).

This lets us:

	•	Generalize to new/unseen items (through text names).
	•	Use powerful pretrained vision-language models (CLIP).
	•	Scale to many item types without training a detector for each ASIN.

# 4. Training Data Construction

Starting from df_items, we construct a training dataset of (image, item_name, required_quantity, label).

For each bin image:
1.	Positive samples (correct item & sufficient quantity)
	
		-	For each item in the bin with bin_quantity:
		-	Sample required quantities q such that 1 ≤ q ≤ bin_quantity.
		-	Label 1 (the bin satisfies the order for that item).
		
2.	Negative samples (correct item but too large quantity)
	
		-	For the same item:
		-	Sample a few q such that q > bin_quantity (e.g. up to 100).
		-	Label 0.
		
3.	Negative samples (wrong item)
   
		-	Pick random items not in the bin.
		-	Sample q (e.g., 1–5).
		-	Label 0.

This yields ~hundreds of thousands of (image, item, quantity, label) samples.


## 5. Model Architecture

The image of the model architecture can be found in the [model_graph.png](./model_graph.png) file

### 5.1 High-Level Architecture

	1.	Image encoder → CLIP Vision Transformer
	2.	Text encoder → CLIP Text Transformer
	3.	Quantity encoder → small MLP
	4.	Classifier → MLP → binary logit

The Image embeddings are generated by CLIP Vision Transformer.
The Text embeddings are generated by CLIP Text Transformer for the item name.
The Quantity embeddings are generated by a neural network to encode the quantity.
These three embeddings are stacked onto each other and converted into a single embedding which is passed into a classifier head, which outputs the probability of match.

## 6. Reasons for choosing this Architecture

- No bounding boxes provided in the dataset, so object detection models (YOLO, Faster-RCNN, DETR) cannot be trained.
- ClIP can generate embeddings for both images and texts.
- CLIP naturally matches images with text, making it ideal for mapping “bin image ↔ item name” pairs.
- The dataset contains 38k+ unique items, so fixed-class classification is impossible; CLIP handles unseen items via text embeddings.
- The quantity MLP allows the model to reason about “how many” even though the dataset lacks per-object annotations.

## 7. Evaluation

### 7.1 Metrics

On the test set, we compute:

	-	Binary Cross-Entropy Loss: 0.2631
	-	Accuracy: 0.8776
	-	Precision: 0.7822
	-	Recall: 0.8092
	-	F1-score: 0.7955

These are computed on per-sample (image, item_name, required_quantity) predictions.

## 8. Streamlit UI – Functionality

The Streamlit frontend simulates a small “ordering + validation” workflow.

### 8.1 Main Features

1.	Item Catalog Dropdown
   
		•	Loads ~38k unique item names from data/unique_item_names.json.
		•	Dropdown lets the user select one item at a time.

2.	Quantity Selector
   
		•	A dropdown (1–20) for selecting quantity.
		•	Max 10 unique items per order.

4.	Order Management
   
-	“Add item” button:
-	Adds the current (item, quantity) pair to the order.
-	Displays the “Current Order” table.
-	“Done” button:
-	Finalizes the order.
-	Randomly selects a bin image (from [bin-images](./bin-images) which has a subset 0f 100 images).
-	Sends each (image, item_name, quantity) to the model.
-	Displays the model outputs in a results table (separate from the order input section).
-	“Clear” button:
-	Resets the order.
-	Clears the selected image and results.
-	Resets UI state (st.session_state).

5.	Bin Image Display
   
		•	Shows a randomly selected bin image under the results.
		•	Displayed at a fixed width and height and centered in the layout.

6.	Model Output Table
   
		•	Columns: Item, Quantity, Match, Confidence
		•	Match column shows:
			•	✅ if model prediction ≥ 0.5
			•	❌ otherwise
