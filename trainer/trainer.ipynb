{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c31cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "from transformers import CLIPProcessor, CLIPModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b434aada",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_DIR = \"bin-images\"   # change this\n",
    "META_DIR  = \"metadata\" # change this\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using device:\", DEVICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8dea9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "\n",
    "meta_files = [f for f in os.listdir(META_DIR) if f.endswith(\".json\")]\n",
    "for meta_file in tqdm(meta_files):\n",
    "    meta_path = os.path.join(META_DIR, meta_file)\n",
    "    with open(meta_path, \"r\") as f:\n",
    "        meta = json.load(f)\n",
    "    \n",
    "    image_id = meta_file.replace(\".json\", \"\")\n",
    "    item_dict = meta.get(\"BIN_FCSKU_DATA\", {})\n",
    "    expected_total_qty = meta.get(\"EXPECTED_QUANTITY\", None)\n",
    "    \n",
    "    for asin, item in item_dict.items():\n",
    "\n",
    "        name = item.get(\"name\")\n",
    "        normalized = item.get(\"normalizedName\")\n",
    "\n",
    "        # ⭐ SKIP THIS ITEM if both names are missing or empty\n",
    "        if (name is None or str(name).strip() == \"\") and (normalized is None or str(normalized).strip() == \"\"):\n",
    "            # print(f\"Skipping item with no name for ASIN {asin} in {image_id}\")\n",
    "            continue\n",
    "        \n",
    "        rows.append({\n",
    "            \"image_id\": image_id,\n",
    "            \"asin\": asin,\n",
    "            \"name\": name,\n",
    "            \"normalized_name\": normalized,\n",
    "            \"bin_quantity\": item.get(\"quantity\", 1),\n",
    "            \"expected_total_qty\": expected_total_qty,\n",
    "        })\n",
    "\n",
    "df_items = pd.DataFrame(rows)\n",
    "print(\"Rows:\", len(df_items))\n",
    "print(\"Unique images:\", df_items[\"image_id\"].nunique())\n",
    "print(\"Unique ASINs:\", df_items[\"asin\"].nunique())\n",
    "df_items.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e088666",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_training_samples(\n",
    "    df_items: pd.DataFrame,\n",
    "    max_required_qty_cap: int = 100\n",
    ") -> pd.DataFrame:\n",
    "\n",
    "    samples = []\n",
    "    \n",
    "    grouped = df_items.groupby(\"image_id\")\n",
    "    all_asins = df_items[\"asin\"].unique().tolist()\n",
    "\n",
    "    for image_id, group in tqdm(grouped, desc=\"Generating samples\"):\n",
    "\n",
    "        asins_in_bin = set(group[\"asin\"].tolist())\n",
    "        other_asins = list(set(all_asins) - asins_in_bin)\n",
    "\n",
    "        # sample 3 random negative ASINs (without replacement)\n",
    "        wrong_asin_samples = random.sample(other_asins, min(3, len(other_asins))) if len(other_asins) > 0 else []\n",
    "\n",
    "        for _, row in group.iterrows():\n",
    "\n",
    "            asin = row[\"asin\"]\n",
    "            name = row[\"name\"] or row[\"normalized_name\"]\n",
    "            bin_qty = int(row[\"bin_quantity\"])\n",
    "\n",
    "            # -----------------------------\n",
    "            # POSITIVE SAMPLES\n",
    "            # -----------------------------\n",
    "            if bin_qty <= 5:\n",
    "                pos_quantities = list(range(1, bin_qty + 1))\n",
    "\n",
    "            else:\n",
    "                # sample 5 unique quantities from 1..bin_qty\n",
    "                pos_quantities = random.sample(range(1, bin_qty + 1), 5)\n",
    "\n",
    "            for q in pos_quantities:\n",
    "                samples.append({\n",
    "                    \"image_id\": image_id,\n",
    "                    \"item_name\": name,\n",
    "                    \"asin\": asin,\n",
    "                    \"required_quantity\": q,\n",
    "                    \"label\": 1\n",
    "                })\n",
    "\n",
    "            # -----------------------------\n",
    "            # NEGATIVE QUANTITY SAMPLES\n",
    "            # -----------------------------\n",
    "            neg_range_start = bin_qty + 1\n",
    "            neg_range_end = max_required_qty_cap\n",
    "\n",
    "            if neg_range_start <= neg_range_end:\n",
    "                possible_neg_quantities = list(range(neg_range_start, neg_range_end + 1))\n",
    "\n",
    "                neg_quantities = (\n",
    "                    random.sample(possible_neg_quantities, \n",
    "                    min(3, len(possible_neg_quantities)))\n",
    "                )\n",
    "\n",
    "                for q in neg_quantities:\n",
    "                    samples.append({\n",
    "                        \"image_id\": image_id,\n",
    "                        \"item_name\": name,\n",
    "                        \"asin\": asin,\n",
    "                        \"required_quantity\": q,\n",
    "                        \"label\": 0\n",
    "                    })\n",
    "\n",
    "        # -----------------------------\n",
    "        # NEGATIVE WRONG-ASIN SAMPLES\n",
    "        # -----------------------------\n",
    "        for neg_asin in wrong_asin_samples:\n",
    "\n",
    "            neg_row = df_items[df_items[\"asin\"] == neg_asin].sample(1).iloc[0]\n",
    "            neg_name = neg_row[\"name\"] or neg_row[\"normalized_name\"]\n",
    "\n",
    "            # random quantity between 1 and 10\n",
    "            q = random.randint(1, 10)\n",
    "\n",
    "            samples.append({\n",
    "                \"image_id\": image_id,\n",
    "                \"item_name\": neg_name,\n",
    "                \"asin\": neg_asin,\n",
    "                \"required_quantity\": q,\n",
    "                \"label\": 0\n",
    "            })\n",
    "\n",
    "    return pd.DataFrame(samples)\n",
    "\n",
    "df_samples = generate_training_samples(df_items)\n",
    "print(\"Total samples:\", len(df_samples))\n",
    "df_samples.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3eed795",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinOrderDataset(Dataset):\n",
    "    def __init__(self, df_samples: pd.DataFrame, image_dir: str, clip_processor: CLIPProcessor):\n",
    "        self.df = df_samples.reset_index(drop=True)\n",
    "        self.image_dir = image_dir\n",
    "        self.processor = clip_processor\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        image_id = row[\"image_id\"]\n",
    "        item_name = row[\"item_name\"]\n",
    "        required_qty = row[\"required_quantity\"]\n",
    "        label = row[\"label\"]\n",
    "        \n",
    "        img_path = os.path.join(self.image_dir, image_id + \".jpg\")\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        \n",
    "        # We'll let CLIPProcessor handle the transform; here we just return raw image, text.\n",
    "        text = item_name\n",
    "        \n",
    "        return {\n",
    "            \"image\": image,\n",
    "            \"text\": text,\n",
    "            \"required_qty\": float(required_qty),\n",
    "            \"label\": float(label)\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99013369",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLIPQuantityMatcher(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        clip_model_name: str = \"openai/clip-vit-base-patch32\",\n",
    "        quantity_dim: int = 32,\n",
    "        hidden_dim: int = 256,\n",
    "        freeze_clip: bool = True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.clip = CLIPModel.from_pretrained(clip_model_name)\n",
    "        self.clip.eval()\n",
    "        self.config = self.clip.config\n",
    "        \n",
    "        if freeze_clip:\n",
    "            for p in self.clip.parameters():\n",
    "                p.requires_grad = False\n",
    "        \n",
    "        embed_dim = self.config.projection_dim  # CLIP projection dim (e.g. 512)\n",
    "        \n",
    "        self.quantity_mlp = nn.Sequential(\n",
    "            nn.Linear(1, quantity_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(quantity_dim, quantity_dim),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(embed_dim * 2 + quantity_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1)  # binary logit\n",
    "        )\n",
    "    \n",
    "    def forward(self, pixel_values, input_ids, attention_mask, quantities):\n",
    "        # CLIP forward\n",
    "        outputs = self.clip(\n",
    "            pixel_values=pixel_values,\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "        )\n",
    "        image_embeds = outputs.image_embeds   # (B, D)\n",
    "        text_embeds  = outputs.text_embeds    # (B, D)\n",
    "        \n",
    "        # quantities: shape (B,) → (B,1)\n",
    "        q = quantities.unsqueeze(-1)  # normalize if you want, e.g. q/10\n",
    "        q_emb = self.quantity_mlp(q)  # (B, quantity_dim)\n",
    "        \n",
    "        x = torch.cat([image_embeds, text_embeds, q_emb], dim=-1)\n",
    "        logits = self.classifier(x).squeeze(-1)  # (B,)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835a9fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_model_name = \"openai/clip-vit-base-patch32\"\n",
    "clip_processor = CLIPProcessor.from_pretrained(clip_model_name)\n",
    "\n",
    "dataset = BinOrderDataset(df_samples, IMAGE_DIR, clip_processor)\n",
    "\n",
    "# Train/val split\n",
    "val_ratio = 0.1\n",
    "test_ratio = 0.2\n",
    "val_size = int(len(dataset) * val_ratio)\n",
    "test_size = int(len(dataset) * test_ratio)\n",
    "train_size = len(dataset) - val_size - test_size\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
    "print(\"Train size:\", len(train_dataset), \"Val size:\", len(val_dataset), \"Test size:\", len(test_dataset))\n",
    "\n",
    "def collate_fn(batch: List[Dict[str, Any]]):\n",
    "    images = [item[\"image\"] for item in batch]\n",
    "    texts  = [item[\"text\"] for item in batch]\n",
    "    qtys   = torch.tensor([item[\"required_qty\"] for item in batch], dtype=torch.float32)\n",
    "    labels = torch.tensor([item[\"label\"] for item in batch], dtype=torch.float32)\n",
    "\n",
    "    for t in texts:\n",
    "        if not isinstance(t, str):\n",
    "            print(\"\\n❌ BAD TEXT DETECTED:\", t, \"TYPE:\", type(t))\n",
    "    \n",
    "    # Use CLIP processor\n",
    "    encoding = clip_processor(\n",
    "        text=texts,\n",
    "        images=images,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "    )\n",
    "    \n",
    "    batch_dict = {\n",
    "        \"pixel_values\": encoding[\"pixel_values\"],\n",
    "        \"input_ids\": encoding[\"input_ids\"],\n",
    "        \"attention_mask\": encoding[\"attention_mask\"],\n",
    "        \"quantities\": qtys,\n",
    "        \"labels\": labels,\n",
    "    }\n",
    "    return batch_dict\n",
    "\n",
    "BATCH_SIZE = 256\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=16,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=16,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=16,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec85a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CLIPQuantityMatcher(\n",
    "    clip_model_name=clip_model_name,\n",
    "    quantity_dim=32,\n",
    "    hidden_dim=256,\n",
    "    freeze_clip=True,   # you can set to False later to fine-tune\n",
    ").to(DEVICE)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.AdamW(\n",
    "    filter(lambda p: p.requires_grad, model.parameters()),\n",
    "    lr=1e-4\n",
    ")\n",
    "\n",
    "def run_epoch(loader, model, optimizer=None):\n",
    "    is_train = optimizer is not None\n",
    "    model.train() if is_train else model.eval()\n",
    "    \n",
    "    total_loss = 0.0\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    \n",
    "    for batch in tqdm(loader, desc=\"Train\" if is_train else \"Val\"):\n",
    "        pixel_values = batch[\"pixel_values\"].to(DEVICE)\n",
    "        input_ids = batch[\"input_ids\"].to(DEVICE)\n",
    "        attention_mask = batch[\"attention_mask\"].to(DEVICE)\n",
    "        quantities = batch[\"quantities\"].to(DEVICE)\n",
    "        labels = batch[\"labels\"].to(DEVICE)\n",
    "        \n",
    "        with torch.set_grad_enabled(is_train):\n",
    "            logits = model(pixel_values, input_ids, attention_mask, quantities)\n",
    "            loss = criterion(logits, labels)\n",
    "            \n",
    "            preds = (torch.sigmoid(logits) > 0.5).float()\n",
    "            correct = (preds == labels).sum().item()\n",
    "            \n",
    "            if is_train:\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item() * labels.size(0)\n",
    "        total_correct += correct\n",
    "        total_samples += labels.size(0)\n",
    "    \n",
    "    avg_loss = total_loss / total_samples\n",
    "    accuracy = total_correct / total_samples\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "EPOCHS = 20\n",
    "best_val_loss = float(\"inf\")\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    print(f\"\\nEpoch {epoch}/{EPOCHS}\")\n",
    "    train_loss, train_acc = run_epoch(train_loader, model, optimizer)\n",
    "    print(f\"  Train loss: {train_loss:.4f}, acc: {train_acc:.4f}\")\n",
    "    \n",
    "    val_loss, val_acc = run_epoch(val_loader, model, optimizer=None)\n",
    "    print(f\"  Val   loss: {val_loss:.4f}, acc: {val_acc:.4f}\")\n",
    "    \n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save({\n",
    "            \"quantity_mlp\": model.quantity_mlp.state_dict(),\n",
    "            \"classifier\": model.classifier.state_dict()},\n",
    "            \"head_weights.pt\")\n",
    "\n",
    "        print(\"  ✅ Saved new best model.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3bc815c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "def evaluate_test_set(model, test_loader, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_samples = 0\n",
    "    \n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader, desc=\"Testing\"):\n",
    "            pixel_values = batch[\"pixel_values\"].to(device)\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            quantities = batch[\"quantities\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "\n",
    "            logits = model(pixel_values, input_ids, attention_mask, quantities)\n",
    "            loss = criterion(logits, labels)\n",
    "\n",
    "            total_loss += loss.item() * labels.size(0)\n",
    "            total_samples += labels.size(0)\n",
    "\n",
    "            preds = torch.sigmoid(logits).cpu().numpy()\n",
    "            preds = (preds >= 0.5).astype(int)\n",
    "\n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    # Compute metrics\n",
    "    avg_loss = total_loss / total_samples\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        all_labels, all_preds, average=\"binary\"\n",
    "    )\n",
    "\n",
    "    return avg_loss, acc, precision, recall, f1\n",
    "\n",
    "test_loss, test_acc, test_prec, test_rec, test_f1 = evaluate_test_set(\n",
    "    model, test_loader, DEVICE\n",
    ")\n",
    "\n",
    "print(\"\\n====== TEST RESULTS ======\")\n",
    "print(f\"Test Loss:      {test_loss:.4f}\")\n",
    "print(f\"Test Accuracy:  {test_acc:.4f}\")\n",
    "print(f\"Precision:      {test_prec:.4f}\")\n",
    "print(f\"Recall:         {test_rec:.4f}\")\n",
    "print(f\"F1 Score:       {test_f1:.4f}\")\n",
    "print(\"==========================\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
